# cf_ai_log-copilot

An AI-powered log investigation copilot built on **Cloudflare Workers + Durable Objects**.

This application analyzes recent logs, computes evidence-based metrics (error rate, top endpoints, latency percentiles), and uses a Large Language Model (LLM) to generate a ranked root-cause report with recommended next actions.

---

## ğŸŒ Live Demo

https://cf-ai-log-copilot.li-xinle.workers.dev

> Note: The deployed version requires an LLM API key configured as a Cloudflare secret.  
> If the key is missing or quota is exceeded, the UI will display a readable error message.

---

## ğŸš€ Features

- **Chat UI** (single-page HTML)
- **Durable Objects state management** (session-based memory)
- **Log analytics engine**
  - Total requests
  - 5xx error rate
  - Top endpoints
  - Top regions
  - Latency p50 / p95
- **LLM integration (external)** via OpenAI Chat Completions API
- **Structured JSON output parsing** with safe fallback handling
- **Production-ready secret management**

---

## ğŸ— Architecture

High-level flow:

1. Browser sends `{ sessionId, message }` to `POST /api/chat`
2. Worker routes the request to a Durable Object instance keyed by `sessionId`
3. Session Durable Object:
   - Loads and updates conversation history
   - Computes log statistics
   - Calls the LLM using stats + conversation context
   - Returns structured report + stats

### Architecture Diagram

Browser
â”‚ POST /api/chat
â–¼
Worker (src/worker.ts)
â”‚ Routes by sessionId
â–¼
Durable Object (src/session-do.ts)
â”‚ state.storage (memory)
â”‚ analyzeLogs()
â”‚ LLM API call
â–¼
Structured JSON response

---

## ğŸ“‚ Repository Structure

cf_ai_log-copilot/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ worker.ts # HTTP routing + UI + Durable Object stub
â”‚ â”œâ”€â”€ session-do.ts # Stateful session logic + LLM integration
â”‚ â”œâ”€â”€ log_analyzer.ts # Log statistics computation engine
â”‚ â”œâ”€â”€ sample_logs.ts # Demo log dataset
â”‚ â””â”€â”€ prompts.ts # LLM prompt builder
â”‚
â”œâ”€â”€ public/ index.html
â”œâ”€â”€ data/ sample-logs.jsonl
â”‚
â”œâ”€â”€ output.png # Example output screenshot
â”‚
â”œâ”€â”€ PROMPTS.md # Prompt documentation (required by assignment)
â”œâ”€â”€ README.md
â”œâ”€â”€ wrangler.toml # Cloudflare Worker configuration
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ package.json
â”œâ”€â”€ package-lock.json
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ .dev.vars # Local development secrets (NOT committed)

---

## ğŸ§  LLM Design

The prompt includes:

- User question
- Conversation history (last N messages)
- Structured log statistics (JSON)
- Strict output schema requirement:
  - `summary`
  - `root_causes` (ranked with evidence)
  - `next_actions`
  - `follow_ups`

The application attempts to parse structured JSON from the model and falls back safely if parsing fails.

---

## ğŸ’» Local Development

### Prerequisites

- Node.js 20+
- Cloudflare Wrangler

### Install dependencies

```bash
npm install

### Create environment file

Create a file named .dev.vars in the project root:
OPENAI_API_KEY=your_openai_api_key_here
Make sure .dev.vars is listed in .gitignore.

### Run locally

```bash
npx wrangler dev

Open the local URL shown in the terminal.

Deploy to Cloudflare
    1. Set production secret
    npx wrangler secret put OPENAI_API_KEY
    Paste your API key when prompted.
    2. Deploy
    npx wrangler deploy

Demo Prompts

Try asking:
    why are 5xx increasing?
    show top endpoints
    what should I check next?
    is this a latency issue or application error?

```bash